# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contents data about Bank Marketing and our goal is to predict if the client will subscribe to a term deposit with the bank.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a model with acuuracy of aproximately: 0.91.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
First the data was loaded into dataset and proper compute infra was created to run with suitable hyperdrive configuration.
Random Sampling was choosen as it supports early termination of low-performance runs. In random sampling, hyperparameter values are randomly selected from the defined search space.

**What are the benefits of the parameter sampler you chose?**
Random Sampling was choosen as it supports early termination of low-performance runs. In random sampling, hyperparameter values are randomly selected from the defined search space with two hyperparameters '--C' (Reqularization Strength) and '--max_iter' (Maximum iterations to converge).

**What are the benefits of the early stopping policy you chose?**
enable_early_stopping = true,  enables early termination if the score is not improving in the short term.
BanditPolicy is an "aggressive" early stopping policy with the meaning that cuts more runs.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best performing model was a model with acuuracy of aproximately .91 with AUC_weight = 0.949 using VotingEnsemble algoritim.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The best performing model with Hyperdrive Pipeline was a model with acuuracy aproximately : 0.91 and with Automl Pipeline, the best performing model was a model with acuuracy approximately :0.91. However, AutoML avoids the need of frequrent changes in script for diffrent algoritihms.
Images of Diffrent Stages of Excecution of Code: https://github.com/ashishsomvanshi/Optimizing_a_Pipeline_in_Azure_Project/tree/master/images

HyperDrive Pipeline:
Classification technique used: Logistic Regression (Collumn to be classifed : y) 
Best Run Selection: Selected out of multiple runs with same algorititm with diffrent hyperparameters.
Data Pre-Prossesing: The data are cleaned with the clean_data() imported from train.py, with rows with missing values dropped and categorical(textual) fields converted to numerical fields.
Confrugration Selected: -
                hyperparameter_sampling = RandomParameterSampling (With '--C' : choice(0.001,0.01,0.1,1,10,20,50,100,200,500,1000) and '--max_iter' : choice(50,75,100,200))
                primary_metric_name = 'Accuracy'
                primary_metric_goal = PrimaryMetricGoal.MAXIMIZE
                max_total_runs = 20
                max_concurrent_runs = 4
                policy =  BanditPolicy (With slack_factor = 0.1, evaluation_interval = 1, delay_evaluation=5) (allowable slack = 0.1, frequency for applying the policy = 1, First    policy evaluation doene after 5 intervals)
                estimator = LogisticRegression based estimator from SKLearn (illustrated in train.py) with 20% labeled data as test data.
With the above selected confugration the Hyperdrive Pipeline shows best results with --c = 50 and --max_iter = 300 giving accuracy of 0.91(approximate).
                
AutoML Pipeline:
Classification technique used: Multiple Alogritims
Best Run Selection: Selected out of multiple runs with diffrent algorititms with there auto generated hyperparameters.
Data Pre-Prossesing: The data are cleaned with the clean_data() imported from train.py, with rows with missing values dropped and categorical(textual) fields converted to numerical fields.
Confugration Selected: -
                experiment_timeout_minutes=30
                task="classification"
                enable_early_stopping = True (Enable early termination if the score is not improving in the short term)
                primary_metric= "AUC_weighted" ( imbalanced data. For example, the AUC_weighted is a primary metric)
                training_data= train_data (Percentage of labeled data as test data auto selected, we have an option to specify this as well in code)
                label_column_name="y" (Collumn to be classified)
                enable_onnx_compatible_models=True (to emable saving output model in ONNX format)
                n_cross_validations= 3
With the above selected confugration the AutoML Pipeline shows best results with VisualEssemble as AUC Weight (.949 aproximate) being Primary Metric with accuracy of .91 (aproximate).
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
In future we can improve the best model by choosing diffrent primary_metrics and diffrent classification methods and claulating and comparing the values of mean_squared_error, to study how our preditiocns have devevated from actual values and also we study mean absolute percent error (MAPE) in detail. We can also study the impact of inncreasing number of clusters used to study to get faster reults. All these could help us in reducing error in our model and also help us to study the model much faster.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
To clean up the resources following steps were followed: - (https://github.com/ashishsomvanshi/Optimizing_a_Pipeline_in_Azure_Project/tree/master/images) 
1. Compute cluster deleted via code.
2. Virtual Machine delteted Manually. 
